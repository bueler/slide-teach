% Copyright 2018-2022  Ed Bueler

\documentclass[10pt,hyperref]{beamer}

\mode<presentation>
{
  \usetheme{Madrid}

  \usecolortheme{beaver}

  \setbeamercovered{transparent}
  
  \setbeamerfont{frametitle}{size=\large}
}

\setbeamercolor*{block title}{bg=red!10}
\setbeamercolor*{block body}{bg=red!5}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\usepackage{empheq}
\usepackage{animate}
\usepackage{xspace}
\usepackage{verbatim,fancyvrb}
\usepackage{hyperref}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\grad}{\nabla}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\ddt}[1]{\ensuremath{\frac{\partial #1}{\partial t}}}
\newcommand{\ddx}[1]{\ensuremath{\frac{\partial #1}{\partial x}}}
\renewcommand{\t}[1]{\texttt{#1}}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
%\newcommand{\MO}{\Matlab/\Octave}
\newcommand{\MO}{\Matlab}
\newcommand{\eps}{\epsilon}

\newcommand{\MS}{\alert{MAKE SURE}\xspace}

\newcommand{\exer}[2]{\medskip\noindent \textbf{#1.}\quad #2}

\newcommand{\mfile}[1]{
\VerbatimInput[frame=single,label=\fbox{\scriptsize \textsl{\,#1\,}},fontfamily=courier,fontsize=\scriptsize]{#1}
}

\newcommand{\mfiletiny}[1]{
\VerbatimInput[frame=single,label=\fbox{\scriptsize \textsl{\,#1\,}},fontfamily=courier,fontsize=\tiny]{#1}
}

\DefineVerbatimEnvironment{mVerb}{Verbatim}{numbersep=2mm,framerule=0.1mm,framesep=2mm,xleftmargin=4mm,fontsize=\small}


\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection,hideallsubsections]
  \end{frame}
}

\title{Steepest descent}

\subtitle{needs help}

\author{Ed Bueler}

\institute[MATH 661]{MATH 661 Optimization}

\date{Fall 2024}

\begin{document}
\beamertemplatenavigationsymbolsempty

\begin{frame}
  \maketitle
\end{frame}


\begin{frame}{steepest descent for unconstrained optimization}

\begin{itemize}
\item these slides are a brief introduction to a well-known topic in unconstrained optimization: \alert{steepest descent}
    \begin{itemize}
    \item[$\circ$] also known as \alert{gradient descent}
    \end{itemize}
\item please read sections 12.1 and 12.2 of the textbook,\footnote{Griva, Nash \& Sofer, \emph{Linear and Nonlinear Optimization}, 2nd ed., SIAM Press 2009} but just ignore the Lemmas for now; we will get back to it
\item codes seen in these slides are already posted at the Codes tab of the public site
\end{itemize}
\end{frame}


\begin{frame}{the steepest descent algorithm}

\begin{itemize}
\item assume $f:\RR^n \to \RR$ has (at least) one continuous derivative
\item we want to solve the unconstrained problem:
    $$\min_{\RR^n} f(x)$$
\item the \alert{steepest descent algorithm}:

\medskip
\mbox{\parbox{\textwidth}{%
\begin{itemize}
\item[1.] User supplies $x_0$.

\medskip
\item[2.] For $k=0,1,2,\dots$
    \begin{itemize}
    \item[(i)] User provides stopping criterion:  If $x_k$ is optimal then stop.
    \item[(ii)] Search direction is $p_k = - \grad f(x_k)$.
    \item[(iii)] User determines step length $\alpha_k>0$.
    \item[(iii)] Let $x_{k+1}=x_k+\alpha_k p_k$.
    \end{itemize}
\end{itemize}%
}}

\end{itemize}
\end{frame}


\begin{frame}{steepest descent is obvious}

\begin{itemize}
\item steepest descent is an obvious interpretation of ```General Optimization Algorithm II'' in \S 2.4
    \begin{itemize}
    \item[$\circ$] direction is chosen as ``go straight downhill''
        \begin{itemize}
        \item the gradient points straight uphill
        \end{itemize}
    \item[$\circ$] \alert{but we don't know how to use the length of $\grad f(x_k)$}
    \item[$\circ$] so we \emph{must} make a nontrivial step-length choice for $\alpha_k$
    \item[$\circ$] also we need a stopping criterion
    \item[$\circ$] and an initial iterate
    \end{itemize}

\medskip
\item \emph{any} choice of steepest descent length ($p_k = - \alpha \grad f(x_k)$ and $\alpha>0$) generates a (feasible) descent direction at $x_k$
    \begin{itemize}
    \item[$\circ$] recall: $p$ is a \emph{descent direction at} $x$ if $p^\top \grad f(x)<0$
    \end{itemize}

\medskip
\item \emph{fun fact:} if $\grad f(x_k)\ne 0$ then the direction of $p_k = - \grad f(x_k)$ solves this optimization problem
    $$\min_{\|q\|=1} q^\top \grad f(x_k)$$
\end{itemize}
\end{frame}


\begin{frame}[fragile]{one way to choose step length: back-tracking}

\begin{itemize}
\item we will see in section 11.5 that we can prove convergence of many unconstrained optimization algorithms as long as the step-size $\alpha_k$ is chosen to satisfy certain conditions
    \begin{itemize}
    \item[$\circ$] this is the \alert{line search} idea
    \end{itemize}
\item for now I just need \emph{some} reasonable way to choose $\alpha_k$
\item the most common way to satisfy these conditions is ``back-tracking''
    \begin{itemize}
    \item[$\circ$] page 378 of the textbook
    \item[$\circ$] an implementation:

\medskip
\begin{Verbatim}[fontsize=\footnotesize]
    function alpha = bt(xk,pk,dfxk,f)
    Dk = dfxk' * pk;     % scalar directional derivative
    mu = 1.0e-4;         % modest sufficient decrease
    rho = 0.5;           % backtracking by halving
    alpha = 1.0;
    while f(xk + alpha * pk) > f(xk) + mu * alpha * Dk
        alpha = rho * alpha;
    end
\end{Verbatim}
    \end{itemize}

\medskip
\item \alert{we will return to this topic}, and prove remarkable Theorem 11.7
\end{itemize}
\end{frame}


\begin{frame}[fragile]{steepest-descent with back-tracking code}

\begin{itemize}
\item here is a basic implementation of \emph{steepest-descent} with \emph{back-tracking}
    \begin{itemize}
    \item[$=$] SDBT
    \end{itemize}
\item it assumes that the user supplies $x_0$ and a function $f$ that returns both the values $f(x)$ and the gradient $\grad f(x)$:

\medskip
\begin{Verbatim}[fontsize=\small]
function [z, xk, k] = sdbt(f, x0, tol)

xk = x0(:);
maxiters = 10000;
for k = 1:maxiters
    [fk, dfk] = f(xk);           % objective and gradient
    if norm(dfk) < tol
        z = fk;
        break                    % success
    end
    pk = - dfk(:);               % steepest descent
    alpha = bt(xk, pk, dfk, f);  % back-tracking
    xk = xk + alpha * pk;
end
\end{Verbatim}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{steepest-descent-back-tracking: example I}

\begin{itemize}
\item suppose $f(x) = 5 x_1^2 + \frac{1}{2} x_2^2$ for $x\in\RR^2$, an easy quadratic objective function with global minimum at $x^*=(0,0)^\top$
\item using the codes:
\begin{mVerb}
>> x0 = [2 20]';   % start far away
>> [z, xk, k] = sdbt(@easyq, x0, 1.0e-10)
z = 1.8601e-21
xk =
   3.6456e-12
   5.9894e-11
k =   105
\end{mVerb}
\item this is based on a function which returns $f(x)$ and $\grad f(x)$:
\begin{Verbatim}[fontsize=\footnotesize]
function [fx, dfx] = easyq(x)

fx = 5 * x(1)^2 + 0.5 * x(2)^2;
dfx = [10 * x(1);
       x(2)];
\end{Verbatim}
\end{itemize}
\end{frame}


\begin{frame}{steepest-descent-back-tracking: example I}

\begin{itemize}
\item recall: $f(x) = 5 x_1^2 + \frac{1}{2} x_2^2$, $x_0=(2,20)^\top$
\item result from SDBT:

\begin{center}
\includegraphics[width=0.5\textwidth]{figs/egg-sd-backtracking}
\end{center}

\item is this result o.k.?
\end{itemize}
\end{frame}


\begin{frame}{steepest-descent-back-tracking: example II}

\begin{itemize}
\item a famously-harder problem in $\RR^2$ is to minimize the \emph{Rosenbrock function}:
    $$f(x) = 100 (x_2 - x_1^2)^2 + (1 - x_1)^2$$
    \begin{itemize}
    \vspace{-4mm}
    \item[$\circ$] a quartic polynomial in 2 variables
    \item[$\circ$] has a single global minimum at $x^*=(1,1)^\top$
    \item[$\circ$] has steep ``banana'' shaped contours (bottom left)
    \end{itemize}

\smallskip
\item at right: SDBT from $x_0=(0,0)^\top$
    \begin{itemize}
    \item[$\circ$] struggles
    \end{itemize}
\end{itemize}

\vspace{-8mm}

\hspace{0.3in}
\includegraphics[width=0.35\textwidth,keepaspectratio=true]{figs/rosencontour}
\hspace{0.3in}
\includegraphics[width=0.46\textwidth,keepaspectratio=true]{figs/sdrosenslow}
\end{frame}


\begin{frame}{quadratic functions}

\begin{itemize}
\item consider general quadratic functions in $\RR^n$
\item such functions can always be written
    $$f(x) = \frac{1}{2} x^\top Q\, x - c^\top x + d$$
    \begin{itemize}
    \vspace{-3mm}
    \item[$\circ$] $Q$ is a symmetric square matrix, $c$ is a column vector, $d\in \RR$
    \item[$\circ$] recall that
        $$\grad f(x) = Q\, x - c$$
    \item[$\circ$] example I above: $c=0$, $d=0$, and $\displaystyle Q = \begin{bmatrix} 5 & 0 \\ 0 & 1/2 \end{bmatrix}$
    \end{itemize}
\item if $Q$ is positive definite then
    \begin{itemize}
    \item $f$ is strictly convex, and
    \item there is unique global minimizer where $\grad f=0$: \qquad $x^* = Q^{-1} c$
    \end{itemize}
\item so quadratic functions are easy to handle, but steepest descent does a bad job!
\end{itemize}
\end{frame}


\begin{frame}{line search for quadratic functions}

\begin{itemize}
\item given any descent direction $p_k$ at $x_k$, for quadratic functions the \emph{optimal} step size is
    $$\alpha_k = \frac{-p_k^\top \grad f(x_k)}{p_k^\top Q p_k} = \frac{p_k^\top (c - Q x_k)}{p_k^\top Q p_k}$$
    \begin{itemize}
    \item[$\circ$] Exercise \textbf{P15} on Assignment \# 7
    \end{itemize}
\item this $\alpha_k$ minimizes $g(\alpha) = f(x_k + \alpha p_k)$ over $\alpha>0$
\item thus back-tracking is \emph{not} needed for quadratic functions

\bigskip
\item \alert{but} steepest descent is still slow
    \begin{itemize}
    \item[$\circ$] Exercise \textbf{P16} asks you to reproduce Example 12.1 in section 12.2 of the textbook.  In this example, steepest descent with optimal step size uses a totally-unnecessary 216 steps to get modest accuracy.
    \item<2>[$\circ$] fundamentally, \alert{the steepest descent direction is wrong},
    \item<2>[$\circ$] and \alert{the steepest descent idea by itself does not address how far to go}
    \end{itemize}

\end{itemize}
\end{frame}


\begin{frame}{steepest descent is the wrong direction}

\begin{itemize}
\item for quadratic objective functions $f(x)=\frac{1}{2} x^\top Q x - c^\top x$,

\centerline{\alert{the Newton iteration converges to $x^*=Q^{-1}c$ in one step}}

\bigskip
\item Newton uses this search direction $p_k$ which solves:
    $$\grad^2 f(x_k)\, p_k = - \grad f(x_k)$$
\item steepest descent uses $p_k$ which solves:
    $$I\, p_k = - \grad f(x_k)$$
\item the identity $I$ is the wrong matrix; for quadratic functions it should be the Hessian of $f$ at $x_k$

\bigskip
\item unconstrained optimization \alert{needs the information in the Hessian $\grad^2 f(x_k)$}, which rotates and scales the steepest descent vector $-\grad f(x_k)$ to be an accurate step toward the minimum
    \begin{itemize}
    \item[$\circ$] that's why it is worth reading Chapters 11, 12, and 13!
    \item[$\circ$] especially ``quasi-Newton'' methods
    \item[$\circ$] however, computing and solving with the Hessian is expensive
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{summary}

\begin{itemize}
\item steepest descent (gradient descent) simply uses the search direction $p_k = -\grad f(x_k)$
\item determining the step size $\alpha_k$, when actually taking the step, namely $x_{k+1}=x_k + \alpha_k p_k$, is nontrivial
    \begin{itemize}
    \item[$\circ$] line search (section 11.5) or trust region (11.6) is needed
    \item[$\circ$] for general functions, back-tracking is reasonable
    \item[$\circ$] for quadratic functions we can use the optimal step size
    \end{itemize}
\item even with good line search, steepest descent sucks
    \begin{itemize}
    \item[$\circ$] steepest descent is slow when contour lines (level sets) are highly curved

    \smallskip
    \item[$\circ$] going down the gradient is generally \alert{the wrong direction}:
        \begin{itemize}
        \item steepest descent direction $p_k = - I^{-1} \grad f(x_k)$ is wrong, while
        \item Newton direction $p_k = - (\grad^2f(x_k))^{-1} \grad f(x_k)$ is perfect for quadratic objectives
        \end{itemize}
    \item[$\circ$] the steepest-descent vector $p_k = -\grad f(x_k)$ has a length which depends on the scaling of $f(x)$, which is bad
        \begin{itemize}
        \item the Newton step does not have this flaw
        \end{itemize}
    \end{itemize}
\item however, functions like Rosenbrock remain difficult even for Newton
\end{itemize}
\end{frame}


\begin{frame}{the other thing you should know \dots}

\begin{itemize}
\item for machine learning (ML) problems, a version called \emph{stochastic gradient descent} (SGD) is the industry baseline
    \begin{itemize}
    \item[$\circ$] Adam, etc.~are based on SGD, but with added ``moment tracking''
    \end{itemize}
\item if ML were actually optimization, as it is usually portrayed, this would be very odd
\item \dots however, ML is not really optimization, but a different game
\item I hope to tell the story by the end of the semester
\end{itemize}
\end{frame}


\end{document}

