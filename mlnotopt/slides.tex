% Copyright 2024  Ed Bueler

\documentclass[10pt,
               svgnames,
               hyperref={colorlinks,citecolor=DeepPink4,linkcolor=FireBrick,urlcolor=Maroon},
               usepdftitle=false]{beamer}

\usetheme{Madrid}
\usecolortheme{beaver}
\setbeamercovered{transparent}
\setbeamerfont{frametitle}{size=\large}
\setbeamercolor*{block title}{bg=red!10}
\setbeamercolor*{block body}{bg=red!5}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\usepackage{empheq,bm,xspace,minted}
%\usepackage{hyperref}
\usepackage{tikz}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bl}{\bm{\ell}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\bzero}{\bm{0}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\ddt}[1]{\ensuremath{\frac{\partial #1}{\partial t}}}
\newcommand{\ddx}[1]{\ensuremath{\frac{\partial #1}{\partial x}}}
\renewcommand{\t}[1]{\texttt{#1}}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\eps}{\epsilon}

\newcommand{\twovect}[4]{\ensuremath{{#1}_{#2} =
                            \begin{bmatrix} #3 \\ #4 \end{bmatrix}}}

\newcommand{\ftt}[1]{{\color{blue} \texttt{#1}}}

\newcommand{\rbullet}{{\color{FireBrick} \bullet}}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}


\title{Machine learning training is not optimization}

\subtitle{but it's close}

\author{Ed Bueler}

\institute[]{UAF Math 661 Optimization}

\date{Fall 2024}


\begin{document}
\beamertemplatenavigationsymbolsempty

\begin{frame}
  \maketitle
\end{frame}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections]
\end{frame}


\section{regression \emph{is} optimization}

\begin{frame}{title}

\begin{itemize}
\item foo
   \begin{itemize}
   \item[$\circ$] bar
   \end{itemize}
\end{itemize}
\end{frame}


\section{generalization}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}

\begin{frame}{title}

\begin{itemize}
\item foo
   \begin{itemize}
   \item[$\circ$] bar
   \end{itemize}
\end{itemize}
\end{frame}


\section{stochastic gradient descent (SGD)}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}

\begin{frame}{title}

\begin{itemize}
\item foo
   \begin{itemize}
   \item[$\circ$] bar
   \end{itemize}
\end{itemize}
\end{frame}


\section{neural nets}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}

\begin{frame}{title}

\begin{itemize}
\item foo
   \begin{itemize}
   \item[$\circ$] bar
   \end{itemize}
\end{itemize}
\end{frame}


\section{online optimization and regret}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}

\begin{frame}{title}

\begin{itemize}
\item foo
   \begin{itemize}
   \item[$\circ$] bar
   \end{itemize}
\end{itemize}
\end{frame}


\section{beyond SGD}

\begin{frame}{Outline}
  \tableofcontents[hideallsubsections,currentsection]
\end{frame}

\begin{frame}{title}

\begin{itemize}
\item foo
   \begin{itemize}
   \item[$\circ$] bar
   \end{itemize}
\end{itemize}
\end{frame}



\begin{frame}[fragile]
\frametitle{fast solution of circulant systems}

\begin{itemize}
\item suppose $A$ is circulant, with first column $c$, and we want to solve $Ax=b$
\item we know
  $$A = c_0 I + c_1 D_m + c_2 D_m^2 + \dots + c_{m-1} D_m^{m-1} = p(D_m)$$
and $D_m = F_m \Lambda F_m^{-1}$ where $\lambda_j = {\bar\omega_m}^j$
\item fast solution process:
    $$Ax = b \qquad \iff \qquad F_m p(\Lambda) F_m^{-1} x = b \qquad \iff \qquad \begin{matrix} u = F_m^{-1} b \\ v = p(\Lambda)^{-1} u \\ x = F_m v \end{matrix}$$ 

\begin{minted}[fontsize=\small]{python}
def solve_circulant(c,x,b):
    z = barFFT(c)
    u = barFFT(b)
    v = u ./ z
    x = FFT(v)
    x /= m
    return x
\end{minted}

\vspace{-20mm}
\hfill $\gets$ \quad $O(m\log m)$ flops \dots optimal!

\vspace{15mm}
\end{itemize}
\end{frame}


\begin{frame}{title}

\begin{itemize}
\item foo
   \begin{itemize}
   \item[$\circ$] bar
   \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{references}

\begin{itemize}
{\small
\item E.~Bueler (2022a). \href{https://bueler.github.io/M692S22/slides/bueler-intro.pdf}{\emph{Getting started on machine learning with one \dots neural net}}
    \begin{itemize}
    \scriptsize
    \item[$-$] slides on neural net basics, and how SGD is applied in practice
    \end{itemize}
\item E.~Bueler (2022b). \href{https://bueler.github.io/M692S22/slides/bueler-regret.pdf}{\emph{Online optimization: good ML training algorithms regret less}}
    \begin{itemize}
    \scriptsize
    \item[$-$] slides on online optimization and regret bounds
    \end{itemize}
\item I.~Goodfellow,  Y.~Bengio, \& A.~Courville (2016).  \href{https://mitpress.mit.edu/9780262035613/deep-learning/}{\emph{Deep Learning}}, MIT Press
    \begin{itemize}
    \scriptsize
    \item[$-$] see sections on optimization and SGD
    \end{itemize}
\item D.~P.~Kingma \& J.~Ba (2014). \href{https://arxiv.org/abs/1412.6980}{\emph{Adam: A method for stochastic optimization}}, preprint arXiv:1412.6980
    \begin{itemize}
    \scriptsize
    \item[$-$] 196,000 citations
    \end{itemize}
\item S.~J.~Reddi, S.~Kale, \& S.~Kumar (2019). \href{https://arxiv.org/abs/1904.09237}{\emph{On the convergence of Adam and beyond}}, preprint arXiv:1904.09237
    \begin{itemize}
    \scriptsize
    \item[$-$] Adam $\stackrel{\text{replace with}}{\to}$ AMSGrad
    \end{itemize}
\item M.~Zinkevich (2003). \href{https://www.aaai.org/Papers/ICML/2003/ICML03-120.pdf}{\emph{Online convex programming and generalized infinitesimal gradient ascent}}, Proceedings of the 20th International Conference on Machine Learning, 928-936
    \begin{itemize}
    \scriptsize
    \item[$-$] introduced regret; $O(\sqrt{j})$ regret bound of SGD
    \end{itemize}
}
\end{itemize}
\end{frame}

\end{document}
