% Copyright 2020  Ed Bueler

\documentclass[10pt,hyperref]{beamer}

\mode<presentation>{
  \usetheme{Madrid}
  \usecolortheme{beaver}
  \setbeamercovered{transparent}
  \setbeamerfont{frametitle}{size=\large}
}

\setbeamercolor*{block title}{bg=red!10}
\setbeamercolor*{block body}{bg=red!5}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

\usepackage{empheq}
\usepackage{xspace}
\usepackage{verbatim,fancyvrb}

%\usepackage[colorlinks=true]{hyperref}
\hypersetup{colorlinks}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bw}{\mathbf{w}}

\newcommand{\grad}{\nabla}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\ddt}[1]{\ensuremath{\frac{\partial #1}{\partial t}}}
\newcommand{\ddx}[1]{\ensuremath{\frac{\partial #1}{\partial x}}}
\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\Octave}{\textsc{Octave}\xspace}
\newcommand{\MO}{\Matlab}
\newcommand{\eps}{\epsilon}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\image}{\operatorname{im}}

\newcommand{\ds}{\displaystyle}

\newcommand{\ip}[2]{\left<#1,#2\right>}

\newcommand{\trefcolumn}[1]{\begin{bmatrix} \phantom{x} \\ #1 \\ \phantom{x} \end{bmatrix}}
\newcommand{\trefmatrixtwo}[2]{\left[\begin{array}{c|c|c} & & \\ #1 & \dots & #2 \\ & & \end{array}\right]}
\newcommand{\trefmatrixthree}[3]{\left[\begin{array}{c|c|c|c} & & & \\ #1 & #2 & \dots & #3 \\ & & & \end{array}\right]}
\newcommand{\trefmatrixgroups}[4]{\left[\begin{array}{c|c|c|c|c|c} & & & & & \\ #1 & \dots & #2 & #3 & \dots & #4 \\ & & & & & \end{array}\right]}

\newcommand{\blocktwo}[4]{\left[\begin{array}{c|c} #1 & #2 \\ \hline #3 & #4 \end{array}\right]}

\newcommand{\bqed}{{\color{blue}\qed}}

\newcommand{\exer}[2]{\medskip\noindent \textbf{#1.}\quad #2}

% I think I want this:
\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection,hideallsubsections]
  \end{frame}
}

\title[Finite-dimensional spectral theory II]{Finite-dimensional spectral theory}

\subtitle{part II: understanding the spectrum (and singular values)}

\author{Ed Bueler}

\institute[MATH 617]{MATH 617 Functional Analysis}

\date{Spring 2020}

\begin{document}
\beamertemplatenavigationsymbolsempty


\begin{frame}
  \maketitle
\end{frame}


\section{introduction}

\begin{frame}{what happened in part I}

\begin{itemize}
\item see part I first: \quad \href{http://bueler.github.io/M617S20/slides1.pdf}{\texttt{bueler.github.io/M617S20/slides1.pdf}}
\item \emph{definition.} for a square matrix $A\in\CC^{n\times n}$, the \emph{spectrum} is the set
    $$\sigma(A)=\left\{\lambda\in\CC\,\big|\,Av=\lambda v \text{ for some }v\ne 0\right\}$$
\item we proved:
    \begin{itemize}
    \item[] $A = Q T Q^*$ \quad \emph{Schur decomposition} \quad for any $A \in \CC^{n\times n}$
    \item[] $A = Q \Lambda Q^*$ \quad \emph{spectral theorem} \quad for normal ($AA^*=A^*A$) matrices
    \end{itemize}
where $Q$ is unitary, $T$ is upper-triangular, and $\Lambda$ is diagonal
    \begin{itemize}
    \item[$\circ$] both decompositions ``reveal'' the spectrum:
      $$\sigma(A) = \{\text{diagonal entries of $T$ or $\Lambda$}\}$$
    \item[$\circ$] spectral theorem for hermitian matrices is sometimes called the \emph{principal axis decomposition} for quadratic forms
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{goal for MATH 617}

\begin{block}{goal}
extend the spectral theorem to $\infty$-dimensions
\end{block}

\begin{itemize}
\item only possible for linear operators on Hilbert spaces $H$
    \begin{itemize}
    \item[$\circ$] inner product needed for adjoints and unitaries
    \item[$\circ$] unitary maps needed because they preserve vector space \emph{and} metric \emph{and} adjoint structures
    \end{itemize}
\item textbook (Muscat) extends to \alert{compact normal operators} on $H$
    \begin{itemize}
    \item[$\circ$] the spectrum is eigenvalues (almost exclusively)
    \end{itemize}
\item recommended text (B.~Hall, \emph{Quantum Theory for Mathematicians}) extends further to \alert{bounded (continuous) normal operators} on $H$
    \begin{itemize}
    \item[$\circ$] spectrum is not only eigenvalues
    \item[$\circ$] statement of theorem uses projector-valued measures
    \end{itemize}
\item Hall also extends to unbounded normal operators on $H$
    \begin{itemize}
    \item[$\circ$] but we won't get there \dots
    \end{itemize}
\item the Schur decomposition has no straightforward extension
\end{itemize}
\end{frame}


%\begin{frame}{table of contents}
%\tableofcontents
%\end{frame}


\begin{frame}{important class: unitary matrices}

\begin{itemize}
\item back to matrices!
\end{itemize}

\begin{definition}
$U \in \CC^{n\times n}$ is \emph{unitary} if $U^*U=I$
\end{definition}

\begin{lemma}
Consider $\CC^n$ as a inner product space with $\ip{v}{w}=v^*w$ and $\|v\|_2 = \sqrt{\ip v v}$.  Suppose $U$ is linear map on $\CC^n$.  The following are equivalent:

\begin{itemize}
\item $U$ is unitary
\item expressed in the standard basis, the columns of $U$ are ON basis of $\CC^n$
\item $\ip{Uv}{Uw}=\ip{v}{w}$ for all $v\in\CC^n$
\item $\|Uv\|_2=\|v\|_2$ for all $v\in\CC^n$
\item $U$ is a metric-space isometry
\end{itemize}
\end{lemma}
\end{frame}


\begin{frame}{important class: normal matrices}

\begin{definition}
$A \in \CC^{n\times n}$ is \emph{normal} if $A^*A=AA^*$
\end{definition}

\begin{itemize}
\item includes: hermitian ($A^*=A$), unitary, skew-hermitian ($A^*=-A$)
\end{itemize}

\begin{lemma}
Consider $\CC^n$ as a inner product space with $\ip{v}{w}=v^*w$ and $\|v\|_2 = \sqrt{\ip v v}$.  Suppose $A$ is linear map on $\CC^n$.  The following are equivalent:

\begin{itemize}
\item $A$ is normal
\item $\|Ax\|_2 = \|A^*x\|_2$ for all $x$
\item exists an ON basis of eigenvectors of $A$
\item exists $Q$ unitary and $\Lambda$ diagonal so that $A=Q\Lambda Q^*$ (\emph{spectral theorem})
\end{itemize}
\end{lemma}
\end{frame}


\section{functional calculus}

\begin{frame}{power series of matrices}

\begin{itemize}
\item suppose $A$ is diagonalizable: $A = S \Lambda S^{-1}$
    \begin{itemize}
    \item[$\circ$] where $S$ is invertible and $\Lambda$ is diagonal
    \item[$\circ$] diagonal entries of $\Lambda$ are eigenvalues of $A$
    \item[$\circ$] if $A$ is normal (e.g.~hermitian) then choose $S=Q$ unitary so $S^{-1}=Q^*$
    \end{itemize}
\item powers of $A$:
    $$A^k = S \Lambda S^{-1} S \Lambda S^{-1} S \Lambda S^{-1} \cdots S \Lambda S^{-1} = S \Lambda^k S^{-1}$$
\item if $f(z)$ is a power series then we can create $f(A)$:
\small
\begin{align*}
f(z) &= \sum_{n=0}^\infty c_n z^n & &\implies & f(A) &= \sum_{n=0}^\infty c_n A^n = S \left(\sum_{n=0}^\infty c_n \Lambda^n\right) S^{-1} \\
     &&&& &= S \begin{bmatrix} f(\lambda_1) & & \\ & \ddots & \\ & & f(\lambda_n) \end{bmatrix} S^{-1}
\end{align*}
\normalsize
\item for example: \qquad \small
$\displaystyle e^{tA} = \sum_{n=0}^\infty \frac{t^n}{n!} A^n =  S \begin{bmatrix} e^{t\lambda_1} & & \\ & \ddots & \\ & & e^{t\lambda_n} \end{bmatrix} S^{-1}$
\end{itemize}
\end{frame}


\begin{frame}{what does ``functional calculus'' mean?}

\begin{itemize}
\item given $A\in\CC^{n\times n}$, a (finite-dimensional) \emph{functional calculus} is algebraic-structure-preserving map from a set of functions $f(z)$ defined on $\CC$ to matrices $f(A)\in\CC^{n\times n}$
\item example: for $f(z)$ analytic,
\small
\begin{align*}
f(z) &= \sum_{n=0}^\infty c_n (z-z_0)^n & &\implies & f(A) &= \sum_{n=0}^\infty c_n (A-z_0 I)^n \\
     &&&& &= S \begin{bmatrix} f(\lambda_1) & & \\ & \ddots & \\ & & f(\lambda_n) \end{bmatrix} S^{-1}
\end{align*}
\normalsize
\item but \dots
    \begin{itemize}
    \item[$\circ$] does the matrix power series $f(A) = \sum_{n=0}^\infty c_n (A-z_0 I)^n$ converge? \textbf{reasonable question}
    \item[$\circ$] does $f(z)$ have to be analytic anyway?
    
     \textbf{no}
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{norms of powers}

\begin{itemize}
\item for any induced norm:
    $$\|A^k\| \le \|A\|^k$$
\item if $A$ is diagonalizable then in any induced norm
    $$\|A^k\| = \|S\Lambda^k S^{-1}\| \le \kappa(S) \max_{\lambda\in\sigma(A)} |\lambda|^k = \kappa(S) \rho(A)^k$$

\vspace{-3mm}
    \begin{itemize}
    \item[$\circ$] $\kappa(S)=\|S\|\|S^{-1}\|$ is the \emph{condition number} of $S$
    \item[$\circ$] $\rho(A)=\max_{\lambda\in\sigma(A)} |\lambda|$ is the \emph{spectral radius} of $A$
    \item[$\circ$] $\rho(A)\le \|A\|$
    \end{itemize}
\item \emph{corollary.} if $A$ is diagonalizable and $\rho(A)<1$ then $A^k \to 0$ as $k\to\infty$
    \begin{itemize}
    \item[$\circ$] actually this holds for all square $A$ \dots use the Schur or Jordan-canonical-form decompositions
    \end{itemize}
\item if $A$ is normal then, because unitaries preserve $2$-norm,
    $$\|A^k\|_2 = \|Q\Lambda^k Q^*\|_2 = \max_{\lambda\in\sigma(A)} |\lambda|^k = \rho(A)^k$$

\vspace{-3mm}
    \begin{itemize}
    \item[$\circ$] thus $\|A^k\|_2 = \|A\|_2^k$
    \item[$\circ$] note $\kappa_2(Q)=1$ for a unitary matrix $Q$
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{convergence when $f(z)$ is analytic}

does it converge? \hspace{10mm} $\ds f(A) \stackrel{\ast}{=} \sum_{n=0}^\infty c_n (A-z_0 I)^n$

\begin{lemma}
Suppose $f(z) = \sum_{n=0}^\infty c_n (z-z_0)^n$ has radius of convergence $R>0$. If $\|A-z_0 I\|<R$ in some induced norm then sum $\ast$ converges in that norm.
\end{lemma}

    \begin{itemize}
    \item[$\circ$] if $A$ is normal then $A = Q \Lambda Q^*$ so
    $$\|A - z_0 I\|_2 = \max_{\lambda\in\sigma(A)} |\lambda-z_0| = \rho(A-z_0 I)$$
    \item[$\circ$] in general $\rho(A-z_0 I) \le \|A-z_0 I\|$ can be strict inequality
    \end{itemize}
\end{frame}


\begin{frame}{defining $f(z)$}

\begin{itemize}
\item compare two ways of defining $f(A)$:
\small
   $$f(A) \stackrel{(1)}{=} \sum_{n=0}^\infty c_n (A-z_0 I)^n \qquad \text{ and } \qquad f(A) \stackrel{(2)}{=} S \begin{bmatrix} f(\lambda_1) & & \\ & \ddots & \\ & & f(\lambda_n) \end{bmatrix} S^{-1}$$
\normalsize
\item for (1) $f$ needs to be analytic and have sufficiently-large radius of convergence relative to norm $\|A-z_0I\|$
\item for formula (2), $A$ needs to be diagonalizable, but $f(z)$ does not need to be analytic \dots it only needs to be defined on $\sigma(A)$
\end{itemize}
\end{frame}


\begin{frame}{the functional calculus for normal matrices}

\begin{theorem}
If $A\in \CC^{n\times n}$ is normal, if $\sigma(A) \subseteq \Omega \subseteq \CC$, and if $f:\Omega \to \CC$, then there is a unique matrix $f(A)\in\CC^n$ so that:
\begin{enumerate}
\item $f(A)$ is normal
\item $f(A)$ commutes with $A$
\item if $Av=\lambda v$ then $f(A)v=f(\lambda)v$
\item $\|f(A)\|_2 = \max_{\lambda\in\sigma(A)} |f(\lambda)|$
\end{enumerate}
\end{theorem}

\emph{proof.}  By the spectral theorem there is a unitary matrix $Q$ and a diagonal matrix $\Lambda$ so that $A=Q\Lambda A^*$, with columns of $Q$ which are eigenvectors of $A$ and all eigenvalues of $A$ listed on the diagonal of $\Lambda$.  Define
    $$f(A) = Q \begin{bmatrix} f(\lambda_1) & & \\ & \ddots & \\ & & f(\lambda_n) \end{bmatrix} Q^*.$$
It has the stated properties.  It is a unique because its action on a basis (eigenvectors of $A$) is determined by property 3.
\end{frame}


\begin{frame}{the meaning of the functional calculus}

\begin{itemize}
\item if $A$ is normal then you can apply any function $f(z)$ to it, giving $f(A)$, as though $A$ is ``just like a complex number''
    \begin{itemize}
    \item[$\circ$] $f$ merely has to be defined\footnote{In $\infty$-dimensions $f$ needs some regularity.  Thus there are separate wikipedia pages on \href{https://en.wikipedia.org/wiki/Holomorphic_functional_calculus}{\emph{holomorphic functional calculus}}, \href{https://en.wikipedia.org/wiki/Continuous_functional_calculus}{\emph{continuous functional calculus}}, and \href{https://en.wikipedia.org/wiki/Borel_functional_calculus}{\emph{borel functional calculus}}.} on the finite set $\sigma(A)$
    \item[$\circ$] the matrix $2$-norm behaves well: $\|f(A)\|_2 = \max_{\lambda\in\sigma(A)} |f(\lambda)|$
    \item[$\circ$] eigendecomposition is therefore powerful when $A$ is normal!
    \end{itemize}
\item if $A$ is diagonalizable then $f(A)$ can be \emph{defined} the same:
\small
   $$f(A) = S \begin{bmatrix} f(\lambda_1) & & \\ & \ddots & \\ & & f(\lambda_n) \end{bmatrix} S^{-1}$$
\normalsize
but surprising behavior is possible: $\|f(A)\| \gg \max_{\lambda\in\sigma(A)} |f(\lambda)|$
\item if $A$ is defective then what?  revert to using power series just to define $f(A)$?
\end{itemize}
\end{frame}


\begin{frame}{functional calculus applications}

\begin{enumerate}
\item suppose $A$ is hermitian and we want to build a unitary matrix from it
    \begin{itemize}
    \item[$\circ$] $A$ is normal and $\sigma(A) \subset \RR$
    \end{itemize}

\medskip
\emph{solution 1.} $f(z) = e^{iz}$ maps $\RR$ to the unit circle so
    $$U = e^{iA} \quad \text{is unitary}$$

\emph{solution 2.} $\displaystyle f(z) = \frac{z+i}{z-i}$ maps $\RR$ to the unit circle so
    $$U = (A+iI)(A-iI)^{-1} \quad \text{is unitary}$$

\medskip
\item suppose $U$ is unitary and we want to build a hermitian matrix from it
    \begin{itemize}
    \item[$\circ$] $U$ is normal and $\sigma(U) \subset S^1 = \{z\in \CC\,:\, |z|=1\}$
    \end{itemize}
\newcommand{\Log}{\operatorname{Log}}

\medskip
\emph{solution.} $f(z) = \Log(z)$ maps the unit circle $S^1$ to the real line, so
    $$A = \frac{1}{i} \Log(U) = -i \Log(U) \quad \text{is hermitian}$$
\end{enumerate}
\end{frame}


\begin{frame}{functional calculus applications: linear ODEs}

\begin{enumerate}
\setcounter{enumi}{2}
\item given $A \in \CC^{n\times n}$ normal, and given $y_0\in\CC$, solve
    $$\frac{dy}{dt} = A y, \qquad y(t_0) = y_0$$
for $y(t) \in \CC^n$ on $t\in [t_0,t_f]$ 

\medskip
\emph{solution.} $y(t) = e^{tz}$ solves $dy/dt=zy$ so, using the functional calculus with $f(z) = e^{(t-t_0)z}$,
\begin{align*}
    y(t) &= e^{(t-t_0)A} y_0 \\
         &= \text{\texttt{expm((t-t0)*A)*y0}}, \\
  \|y(t)\|_2 &= e^{(t-t_0)\omega(A)}\|y_0\|_2
\end{align*}
where $\omega(A) = \max_{\lambda\in\sigma(A)} \operatorname{Re} \lambda$
\begin{itemize}
\item if $A$ is diagonalizable $A=S \Lambda S^{-1}$ then the same applies \dots except the norm of the solution includes $\kappa(S)$
\item if $A$ is defective then the general solution of the ODE system is \emph{not} exponential
\end{itemize}
\item $\infty$-dimensional version: Schr\"odinger's equation in quantum mechanics
\end{enumerate}
\end{frame}


\section{resolvents}

\begin{frame}{resolvents}

\begin{definition}
given $A\in\CC^{n\times n}$ then $\CC\setminus \sigma(A)$ is the \emph{resolvent set}, and if $z \in \CC\setminus \sigma(A)$ then
    $$R_z(A) = \left(A-z I\right)^{-1}$$
is the \emph{resolvent matrix}
\end{definition}

\begin{itemize}
\item recall: $z \in \sigma(A)$ if and only if $A-z I$ is not invertible
\item the resolvent set $\CC\setminus \sigma(A)$ is open
\item $R_0(A)=A^{-1}$ if $0\notin\sigma(A)$
\item $R_z(A)$ ``resolves'' the equation $Av-z v=b$
\end{itemize}
\end{frame}


\begin{frame}{resolvent norms}

\begin{itemize}
\item if $A=S\Lambda S^{-1}$ is diagonalizable and $z\in \CC\setminus \sigma(A)$ then
    $$R_z(A) = \left(S\Lambda S^{-1}-z S I S^{-1}\right)^{-1} = S \left(\Lambda - z I\right)^{-1} S^{-1}$$
so in any induced norm
    $$\|R_z(A)\| = \|S\|\|S^{-1}\|\|\left(\Lambda - z I\right)^{-1}\| = \kappa(S) \max_{\lambda\in\sigma(A)} |\lambda-z|^{-1}$$
\item if $A$ is normal then we can choose $S=Q$ unitary with $\kappa_2(Q)=1$ so
    $$\|R_z(A)\|_2 = \max_{\lambda\in\sigma(A)} |\lambda-z|^{-1}$$
\item one may plot $g(z)=\|R_z(A)\|$

\vspace{-5mm}
\hfill \includegraphics[width=0.45\textwidth]{figs/resolvesurf}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{resolvent norms illustrated}

\small
\begin{itemize}
\item contours of $z\mapsto\|R_z(A)\|_2= \|(A-z I)^{-1}\|_2$ is best spectral picture?

\begin{Verbatim}[fontsize=\footnotesize]
   >> [A,B] = gennormal(5);  % A,B have same eigs; A normal but B not
   >> resolveshow(A)         % normal case     (LEFT)
   >> resolveshow(B)         % nonnormal case  (RIGHT)
\end{Verbatim}

\begin{center}
\includegraphics[width=0.31\textwidth]{figs/resolvenormal} \hspace{20mm} \includegraphics[width=0.3\textwidth]{figs/resolvenonnormal}
\end{center}

\item last slide already proved contours would be round for normal $A$
\item $\sigma_\eps(A) = \left\{z\in\CC\,:\, \|(A-z I)^{-1}\|_2 \ge \eps^{-1}\right\}$ is the $\eps$-\emph{pseudospectrum} of $A$
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{nonnormal matrices, a warning}

\begin{itemize}
\item facts and definitions:
    \begin{itemize}
    \item[$\circ$] $\|A^k\|\le \|A\|^k$ in any induced norm
    \item[$\circ$] $\rho(A) = \max_{\lambda\in\sigma(A)} |\lambda|$
    \item[$\circ$] if $A$ is normal then $\|A^k\|_2 = (\|A\|_2)^k = \rho(A)^k$
    \item[$\circ$] if $\rho(A)<1$ then $A^k \to 0$ as $k\to \infty$ \hfill \emph{proof?}
    \end{itemize}
\item but if $A$ is not normal and $\rho(A)<1$ then $\|A^k\|_2$ \emph{can be big} for a while
    \begin{itemize}
    \item[$\circ$] e.g.~random $100\times 100$ matrices $A$,$B$ with $\rho(A)=\rho(B)<1$
    \end{itemize}
\end{itemize}

\bigskip
\begin{Verbatim}[fontsize=\footnotesize]
   >> max(abs(eig(A)))
   ans =  0.90909
   >> max(abs(eig(B)))
   ans =  0.90909
\end{Verbatim}

\vspace{-17mm}
\hfill \includegraphics[width=0.45\textwidth]{figs/normpowers} \quad \phantom{foo}
\end{frame}


\begin{frame}{redefining ``spectrum'': nonexistence of resolvent}

\begin{definition}
given $A\in\CC^{n\times n}$, the \emph{spectrum of} $A$ is the set
    $$\sigma(A) = \left\{\lambda\in\CC\,\big|\,A-\lambda I \,\text{ does not have a bounded inverse} \right\}$$
\end{definition}

\begin{itemize}
\item in $\CC^n$ this is the same as our original definition:
    $$\sigma(A)=\left\{\lambda\in\CC\,\big|\,Av=\lambda v \text{ for some }v\ne 0\right\}$$
\item in $\infty$-dimensions it is different because there exist one-to-one bounded operators which do not have bounded inverses
    \begin{itemize}
    \item[$\circ$] \emph{example 1}: the one-to-one right-shift operator $R$ on $\ell^1$ has spectrum\footnote{we will prove this by showing that $\sigma(L) = \{z\in\CC\,:\,|z|\le 1\}$ for the left-shift operator $L=R^*$, based on eigenvalues, and that $\sigma(A^*)=\sigma(A)$ in a Banach algebra} $\sigma(R) = \{z\in\CC\,:\,|z|\le 1\}$, but it has no eigenvalues
    \item[$\circ$] \emph{example 2}: the hermitian multiplication operator $(M f)(x) = x f(x)$ on $L^2[0,1]$ has no eigenvalues but $\sigma(M) = [0,1]$
    \end{itemize}
\end{itemize}
\end{frame}


\section{orthogonal projectors}

\begin{frame}{orthogonal projectors}

\begin{definition}
$P \in \CC^{n\times n}$ is an \emph{orthogonal projector} if $P^2=P$ and $P^*=P$
\end{definition}

\begin{itemize}
\item as for any projector ($P^2=P$):
    $$\ker P = \image (I-P), \quad \image P = \ker (I-P), \quad \CC^n = \ker P \oplus \image P, \quad \sigma(P) \subset \{0,1\}$$
\item but for orthogonal projectors:
    $$\ker P \perp \image P$$

    \begin{itemize}
    \item[$\circ$] \emph{proof.}  if $u\in\ker P$ and $v=Pz\in\image P$ then $u^*v = u^*(Pz)=(Pu)^*z = 0$
    \end{itemize}
\item orthogonal projectors are hermitian, thus normal
\item examples:
    $$0, \quad I, \quad P = \begin{bmatrix} 1 & & \\ & 1 & \\ & & 0 \end{bmatrix}$$
\end{itemize}
\end{frame}


\begin{frame}{constructing orthogonal projectors from ON vectors}

\begin{itemize}
\item since $P$ is hermitian and $\sigma(P) \subset \{0,1\}$, the spectral theorem plus re-ordering of the columns of $Q$ gives
    $$P=Q \Lambda Q^* = Q \begin{bmatrix} \,\hat I & \\ & 0 \end{bmatrix} Q^* = \hat Q \hat Q^*$$
where $\hat I$ is a $k\times k$ identity and $\hat Q$ is a $n\times k$ matrix of columns of $Q$

\begin{lemma}  $P\in \CC^{n\times n}$ is an orthogonal projector if and only if there exist ON vectors $q_1,\dots,q_k$, for $0 \le k \le n$, so that

\vspace{-3mm}
    $$P = \hat Q \hat Q^* \quad \text{ and } \quad \hat Q = \trefmatrixthree{q_1}{q_2}{q_k} \in \CC^{n\times k}$$
\end{lemma}
\item hard direction of proof is above; easy direction: $(\hat Q\hat Q^*)^2 = \dots$
\item note $\hat Q^* \hat Q = \hat I$
\item rank 1 case: \quad $P=q q^* = (aa^*)/(a^*a)$
\item construction from full-column-rank $A$: \quad $P = A (A^* A)^{-1} A^*$
\end{itemize}
\end{frame}


\begin{frame}{spectral theorem $=$ decomposition into projectors}

\begin{itemize}
\item consider this calculation for $A$ normal:
\small
\begin{align*}
A &= Q\Lambda Q^* = Q \left(\begin{bmatrix} \lambda_1 & & & \\ & \lambda_2 & & \\ & & \ddots & \\ & & & \lambda_n \end{bmatrix}\right) Q^* \\
  &= Q \left(\begin{bmatrix} \lambda_1 & & \\ & & \\ & & \end{bmatrix} + \dots + \begin{bmatrix} & & \\ & & \\ & & \lambda_n \end{bmatrix}\right) Q^* = q_1 \lambda_1 q_1^* + \dots + q_n \lambda_n q_n^* \\
  &= \sum_{j=1}^n \lambda_j q_j q_j^*
\end{align*}
\normalsize

    \vspace{-2mm}
    \begin{itemize}
    \item[$\circ$] $A$ decomposes into a linear combination of rank-one orthogonal projectors
    \end{itemize}

\medskip
\item thus normal matrices act on vectors like this:
    $$A v = \sum_{j=1}^n \lambda_j q_j q_j^*v = \sum_{j=1}^n \lambda_j \ip{q_j}{v} q_j$$

    \vspace{-2mm}
    \begin{itemize}
    \item[$\circ$] this formula appears in most applications of normal operators
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{resolution of the identity}

\begin{itemize}
\item if $A$ is normal then $\ds A = \sum_{i=1}^n \lambda_i q_i q_i^*$ where $\{q_i\}$ are ON
\item if $A$ is normal then we can use its eigenvectors to decompose the identity:
    $$I = Q Q^* = \sum_{i=1}^n q_i q_i^*$$

    \vspace{-2mm}
    \begin{itemize}
    \item[$\circ$] called a \href{https://en.wikipedia.org/wiki/Spectral_theory\#Resolution_of_the_identity}{\emph{resolution of the identity}}
    \end{itemize}
\item application: \href{https://en.wikipedia.org/wiki/Parseval's_identity}{Parseval's identity} for any ON basis
    $$\|v\|_2^2 = v^* v = v^* I v = \sum_{i=1}^n v^* q_i q_i^* v = \sum_{i=1}^n |\ip{q_i}{v}|^2$$
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{spectra of big random matrices}

\begin{itemize}
\item \emph{claim (\href{https://en.wikipedia.org/wiki/Circular_law}{circular law})}.  if $A\in \RR^{n\times n}$ has entries which are normally-distributed random variables with mean zero and variance $n^{-1}$, so $\ds a_{ij} \sim N(0,n^{-1})$, then as $n\to\infty$ the spectrum of $A$ fills the unit disc

\medskip
\begin{Verbatim}[fontsize=\footnotesize]
>> A = randn(n,n)/sqrt(n);
>> lam = eig(A);
>> plot(real(lam),imag(lam),'o'),  grid on,  axis([-2 2 -2 2])
\end{Verbatim}

\bigskip
\mbox{\includegraphics[width=0.25\textwidth]{figs/randnA16} \qquad \includegraphics[width=0.25\textwidth]{figs/randnA100} \qquad \includegraphics[width=0.25\textwidth]{figs/randnA400}}

\medskip
\item but these matrices are not normal
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{spectra of big random \emph{normal} matrices}

\begin{itemize}
\item but \texttt{randn(n,n)} is not normal (i.e.~normal with probablility zero)
\item construct a random \emph{normal} matrix with the same spectrum:

\bigskip
\VerbatimInput[fontsize=\scriptsize]{../../bueler.github.io/M617S20/matlab/gennormal.m}

\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{spectral subsets correspond to orthogonal projectors}

\begin{itemize}
\item I also wrote a code \, \texttt{projmeasure.m} \, which shows $\sigma(A)$ as a subset of $\CC$ and lets you select the eigenvalues for which you want eigenvectors
\item demo 1:

\begin{Verbatim}[fontsize=\small]
>> A = gennormal(100);
>> P = projmeasure(A);   % <-- user input with mouse
                         %     selects a projector
>> k = rank(P)           % = number of selected eigenvalues
\end{Verbatim}

\item demo 2:

\begin{Verbatim}[fontsize=\small]
>> A = expm(i*eye(6) + gennormal(6));
>> [P,Qh] = projmeasure(A);
>> Qh                    % view selected eigenvectors
\end{Verbatim}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{spectral subsets correspond to orthogonal projectors, cont.}

\begin{itemize}
\item demo 3:

\begin{Verbatim}[fontsize=\small]
>> U = expm(i*genherm(10));  % random unitary matrix
>> [P,Qh] = projmeasure(U);
>> Qh                        % view selected eigenvectors
\end{Verbatim}

\bigskip
\item John von Neumann ($\sim$1930) imagined this before he invented computers
    \begin{itemize}
    \item[$\circ$] it is a \emph{projector-valued measure}
    \item[$\circ$] built to handle quantum mechanical operators rigorously
    \item[$\circ$] he constructed a spectral theorem for normal operators on Hilbert spaces,  which uses a projector-valued measure $E$ on $\CC$, namely
    	$$A = \int_{\sigma(A)} \lambda\,dE_\lambda$$
    \item[$\circ$] the functional calculus:
    	$$f(A) = \int_{\sigma(A)} f(\lambda)\,dE_\lambda,$$
    \end{itemize}

\end{itemize}
\end{frame}


\section{singular value decomposition}

\begin{frame}{why singular values?}

\begin{itemize}
\item eigenvalues can be useful!
\item but they are only defined for square matrices
    \begin{itemize}
    \item[$\circ$] in $\infty$-dimensions: ``spectrum is useful, but only for $B(X)$, not $B(X,Y)$''
    \end{itemize}
\item \dots and sometimes not so useful anyway
    \begin{itemize}
    \item[$\circ$] only ``safe'' to use eigenvalues if eigenvectors are orthogonal ($A$ normal)
    \item[$\circ$] diagonalization $A=S\Lambda S^{-1}$ may tell us little about $A$ when $\kappa(S)\gg 1$
    \item[$\circ$] square matrices can be defective anyway
    \end{itemize}
\item however, \emph{any} $A \in \CC^{m\times n}$ has \emph{singular values}
    \begin{itemize}

\medskip
    \item[$\circ$] what do the \alert{eigenvalues} say?

\medskip
    Behavior of powers $A^k$ or functions $f(A)$ like $e^{At}$.

\medskip
    \item[$\circ$] what do the \alert{singular values} say?

\medskip
Invertibility of $A$: rank, nullity

\medskip
Geometric action of $A$: $\|A\|_2$, $\|A^{-1}\|_2$, condition number, $\eps$-pseudospectrum

\medskip
    \item[$\circ$] so, what information do you want?
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{visualizing a matrix}

\begin{center}
\includegraphics[width=0.7\textwidth]{figs/svd2d}
\end{center}

\vspace{-5mm}
\hfill \tiny \emph{figure from Trefethen \& Bau, \emph{Numerical Linear Algebra}, SIAM Press 1997} \normalsize

\bigskip
\begin{itemize}
\item $A \in \RR^{m\times n}$ sends the unit sphere in $\RR^n$ to a possibly-degenerate hyperellipsoid in $\RR^m$
    \begin{itemize}
    \item[$\circ$] \alert{the fundamental way to visualize a linear operator}
    \item[$\circ$] also true for $A \in \CC^{m\times n}$ \dots but less visualizable
    \end{itemize}
\item the \emph{singular values} of $A$ define the geometry of the output hyperellipsoid
\end{itemize}
\end{frame}


\begin{frame}{singular value decomposition}

\begin{theorem}
 if $A\in \CC^{m\times n}$ then there exist $U\in \CC^{m\times m}$ unitary, $V \in \CC^{n\times n}$ unitary, and $\Sigma \in \RR^{m\times n}$ diagonal, with nonnegative entries, so that
    $$A = U \Sigma V^*$$
\end{theorem}

\begin{itemize}
\item \emph{singular value decomposition (SVD)} of $A$
\item diagonal entries $\sigma_i$ of $\Sigma$ are the \emph{singular values} of $A$
    \begin{itemize}
    \item[$\circ$] note $\Sigma$ is same shape as $A$, while $U,V$ are always square
    \item[$\circ$] normalization $\sigma_1\ge \sigma_2 \ge \dots \ge \sigma_{\min\{m,n\}}$ makes values unique
    \item[$\circ$] if $A=0$ we take $\Sigma=A=0$ and choose $U,V$ as any unitaries
    \item[$\circ$] if $A\ne 0$ then $\sigma_1>0$
    \end{itemize}
\item action of $A = U \Sigma V^*$ on a vector:
    \begin{itemize}
    \item[$\circ$] multiplication by $V^*$ finds coefficients of the vector in the columns of $V$
    \item[$\circ$] multiplication by $\Sigma$ stretches the vector along standard axes
    \item[$\circ$] multiplication by $U$ rotates the vector to the output hyperellipsoid
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{singular value decomposition: examples}

\begin{itemize}
\small
\item \emph{example 1}. if $\ds A = \begin{bmatrix} 4 & 3 \\ 1 & 2 \end{bmatrix}$ then
    $$A =
\begin{bmatrix}    -0.92388  &  -0.38268 \\  -0.38268   &  0.92388 \end{bmatrix}
\begin{bmatrix}      5.3983  &      \\   &  0.92621  \end{bmatrix}
\begin{bmatrix}    -0.75545  & -0.6552 \\  -0.6552 & 0.75545 \end{bmatrix}^*$$
    \begin{itemize}
    \item[$\circ$] $\|A\|_2 = 5.3983$, $\|A^{-1}\|_2 = 1/0.92621$
    \item[$\circ$] compare: $\sigma(A) = \{5,1\}$
    \end{itemize}

\bigskip
\item \emph{example 2}. if $\ds B = \begin{bmatrix} 6 & 5 \\ 4 & 3 \\ 1 & 2 \end{bmatrix}$ then
\footnotesize
    $$B =
\begin{bmatrix} -0.82264 & -0.05242 & -0.56614 \\
                -0.52578 & -0.30878 & 0.79259 \\
                -0.21636 & 0.94969 & 0.22646 \end{bmatrix}
\begin{bmatrix} 9.49393  & \\ & 0.93025 \\ & \end{bmatrix}
\begin{bmatrix} -0.76421 & -0.64497 \\ -0.64497 & 0.76421 \end{bmatrix}^*$$
\small
    \begin{itemize}
    \item[$\circ$] $\|B\|_2 = 9.49393$
    \item[$\circ$] $B$ is not invertible
    \item[$\circ$] $\sigma(B)$ is not defined
    \end{itemize}

\end{itemize}
\end{frame}


\begin{frame}{singular value decomposition: proof}

\begin{itemize}
\footnotesize
\item[\emph{proof.}] Induct on $n$, the column size of $A$.  If $n=1$ then $A=[a]$ where $a\in \CC^m$.  Then
  $$U=\left[\frac{a}{\|a\|_2}\right], \quad \Sigma=[\|a\|_2], \quad V=[1]$$
is an SVD for $A$.

\quad For $n>1$ let $v_1 \in \CC^n$ be a unit vector which maximizes the continuous function
	$$f(x) = \|A x\|_2$$
over the compact set $S^n = \{x\in\CC^n\,:\,\|x\|_2=1\}$.  (\emph{We just used finite-dimensionality!})  Then $Av_1$ is a vector in $\CC^m$ with length $\sigma_1=\|Av_1\|_2 = \|A\|_2$.  If $\sigma_1=0$ we are done because $A$ is the zero matrix.  (\emph{Why?})  Otherwise $\sigma_1>0$ so let $u_1=Av_1/\sigma_1$.  Now we have $Av_1=\sigma_1 u_1$.

\quad Extend $v_1$ and $u_1$ to orthonormal bases of $\CC^n,\CC^m$, respectively, giving unitary matrices
    $$\tilde V = \trefmatrixthree{v_1}{\tilde v_2}{\tilde v_n}, \qquad\tilde U = \trefmatrixthree{u_1}{\tilde u_2}{\tilde u_m}.$$
Now apply $A$ to $\tilde V$,
    $$A\tilde V = \trefmatrixthree{\sigma_1 u_1}{w_2}{w_n}.$$
Next apply $\tilde U^*$, and note that $\tilde U^* u_1 = e_1$:
    $$\tilde U^*A\tilde V = \blocktwo{\sigma_1}{z^*}{0}{M}$$
\normalsize
\end{itemize}
\end{frame}


\begin{frame}{singular value decomposition: proof cont.}

\begin{itemize}
\footnotesize
\item[\emph{cont.}] We have
    $$\tilde U^*A\tilde V = \blocktwo{\sigma_1}{z^*}{0}{M}$$
for $z\in \CC^{n-1}$ and $M \in \CC^{(m-1)\times(n-1)}$.  Because $\tilde U,\tilde V$ are unitary, the matrix norm is unchanged: $\|\tilde U^*A\tilde V\|_2=\|A\|_2$.

\quad In fact $z=0$, as follows.  Let $w\in \CC^m$ be the vector $\ds w = \begin{bmatrix} \sigma_1 \\ z \end{bmatrix}$.  It is nonzero because $\|w\|_2 = (\sigma_1^2 + \|z\|_2^2)^{1/2} \ge \sigma_1>0$.  But
	$$\left\|\blocktwo{\sigma_1}{z^*}{0}{M} \begin{bmatrix} \sigma_1 \\ z \end{bmatrix}\right\|_2 = \left\|\begin{bmatrix} \sigma_1^2 + z^*z \\ Mz \end{bmatrix}\right\|_2 \ge \sigma_1^2 + \|z\|_2^2 = (\sigma_1^2 + \|z\|_2^2)^{1/2} \|w\|_2.$$
That is, $\|\tilde U^*A\tilde V w\|_2 \ge (\sigma_1^2 + \|z\|_2^2)^{1/2} \|w\|_2$, so if $z\ne 0$ then $\|A\|_2=\|\tilde U^*A\tilde V\|_2 > \sigma_1$, contradicting the definition of $\sigma_1$.

\quad Thus 
    $$\tilde U^*A\tilde V = \blocktwo{\sigma_1}{0}{0}{M}$$
By the induction hypothesis there exist $\hat U,\hat\Sigma,\hat V$ so that $M = \hat U \hat\Sigma \hat V^*$.  Since products of unitaries are unitary, we have an SVD of $A$:
    $$A = \left(\tilde U\blocktwo{1}{0}{0}{\hat U}\right) \blocktwo{\sigma_1}{0}{0}{\hat\Sigma} \left(\tilde V \blocktwo{1}{0}{0}{\hat V}\right)^* = U \Sigma V^*\hfill \bqed$$
\normalsize
\end{itemize}
\end{frame}


\begin{frame}{singular value decomposition: facts}

\begin{itemize}
\item $\|A\|_2=\|\Sigma\|_2=\sigma_1$
\item $\alpha$ is a singular value of $A$ if and only if $\alpha^2$ is an eigenvalue of $A^*A$
\item the singular values of $A$ are the same as those of $A^*$
\item for any $A\in \CC^{m\times n}$,
    \begin{itemize}
    \item[$\circ$] $\rank(A)=k$ where $\sigma_k>0$ and $\sigma_{k+1}=0$
    \item[$\circ$] $\operatorname{nullity}(A)=\dim(\ker(A))=q$ where $q$ is the number of zero singular values
    \end{itemize}
\item if $A\in \CC^{n\times n}$ is square then
    \begin{itemize}
    \item[$\circ$] $|\det(A)| = \prod_{j=1}^n \sigma_j$
    \item[$\circ$] if $A$ is invertible then $\|A^{-1}\|_2 = 1/\sigma_n$
    \item[$\circ$] $\kappa_2(A) = \sigma_1/\sigma_n \in [1,\infty]$ is the eccentricity of the output hyperellipsoid
    \item[$\circ$] $\sigma_n \le \min_{\lambda\in\sigma(A)} |\lambda| \le \max_{\lambda\in\sigma(A)} |\lambda| \le \sigma_1$
    \end{itemize}
\item if $A$ is square and normal then $\sigma_j = |\lambda_j|$ (with ordering of $\sigma(A)$)
\end{itemize}
\end{frame}


\section{conclusion}

\begin{frame}{please read the textbook backwards}

\begin{itemize}
\item go to the end of Chapter 15 ``$C^*$ algebras'' and read backwards:
    \begin{itemize}
    \item[$\circ$] von Neumann's spectral theorem for bounded operators on Hilbert spaces
    \item[$\circ$] functional calculus for normal elements
    \item[$\circ$] singular value decomposition for compact operators between Hilbert spaces
    \item[$\circ$] spectral theorem for compact normal operators on a Hilbert space
    \item[$\circ$] definition of \emph{normal}, \emph{unitary}, and \emph{self-adjoint} (hermitian) elements
    \item[$\circ$] definition of a $C^*$ algebra
    \end{itemize}
\item on the other hand, go to the beginning of Chapter 14 ``Spectral theory'' and read forward
\item I hope that by the end of the semester it will make sense!
\end{itemize}
\end{frame}

\end{document}
